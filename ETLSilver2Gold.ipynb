{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "366d2c91-c5ba-4b83-9686-3da1333c61bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SILVER → GOLD ETL  (Spark Connect Safe, Optimized)\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, current_timestamp, current_date,\n",
    "    to_date, sum as F_sum, countDistinct, max as F_max, when\n",
    ")\n",
    "from pyspark.sql import DataFrame\n",
    "import uuid, traceback\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "CATALOG = \"leelastestdata\"\n",
    "SCHEMA  = \"default\"\n",
    "\n",
    "# Silver tables\n",
    "SILVER = {\n",
    "    \"loans\":        f\"{CATALOG}.{SCHEMA}.loans_silver\",\n",
    "    \"payments\":     f\"{CATALOG}.{SCHEMA}.payments_silver\",\n",
    "    \"customers\":    f\"{CATALOG}.{SCHEMA}.customers_silver\",\n",
    "    \"bureau\":       f\"{CATALOG}.{SCHEMA}.bureau_silver\",\n",
    "    \"collections\":  f\"{CATALOG}.{SCHEMA}.collections_silver\",\n",
    "    \"recoveries\":   f\"{CATALOG}.{SCHEMA}.recoveries_silver\"\n",
    "}\n",
    "\n",
    "# Gold control tables\n",
    "GOLD_META_TABLE   = f\"{CATALOG}.{SCHEMA}.gold_last_processed\"\n",
    "GOLD_RUNLOG_TABLE = f\"{CATALOG}.{SCHEMA}.gold_run_log\"\n",
    "\n",
    "GOLD_PARTITION_COL = \"load_date\"\n",
    "\n",
    "\n",
    "# ---------------- HELPERS ----------------\n",
    "\n",
    "def now_ts():\n",
    "    \"\"\"Return Python timestamp (Spark Connect safe).\"\"\"\n",
    "    return spark.sql(\"select current_timestamp() as ts\").first().ts\n",
    "\n",
    "\n",
    "def ensure_gold_control_tables():\n",
    "    spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "    spark.sql(f\"USE SCHEMA {SCHEMA}\")\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {GOLD_META_TABLE} (\n",
    "        gold_table string,\n",
    "        last_processed_timestamp timestamp,\n",
    "        last_updated timestamp\n",
    "    ) USING DELTA\n",
    "    \"\"\")\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {GOLD_RUNLOG_TABLE} (\n",
    "        run_id string,\n",
    "        gold_table string,\n",
    "        start_ts timestamp,\n",
    "        end_ts timestamp,\n",
    "        status string,\n",
    "        rows_written long,\n",
    "        message string\n",
    "    ) USING DELTA\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def write_gold_runlog(run_id, gold_table, start_ts, end_ts, status, rows_written, message):\n",
    "    schema = \"\"\"\n",
    "      run_id string,\n",
    "      gold_table string,\n",
    "      start_ts timestamp,\n",
    "      end_ts timestamp,\n",
    "      status string,\n",
    "      rows_written long,\n",
    "      message string\n",
    "    \"\"\"\n",
    "    df = spark.createDataFrame(\n",
    "        [(run_id, gold_table, start_ts, end_ts, status, rows_written, message)],\n",
    "        schema=schema\n",
    "    )\n",
    "    df.write.format(\"delta\").mode(\"append\").saveAsTable(GOLD_RUNLOG_TABLE)\n",
    "\n",
    "\n",
    "def update_gold_meta(gold_table, ts):\n",
    "    schema = \"\"\"\n",
    "      gold_table string,\n",
    "      last_processed_timestamp timestamp,\n",
    "      last_updated timestamp\n",
    "    \"\"\"\n",
    "    df = spark.createDataFrame(\n",
    "        [(gold_table, ts, now_ts())],\n",
    "        schema=schema\n",
    "    )\n",
    "    spark.sql(f\"DELETE FROM {GOLD_META_TABLE} WHERE gold_table = '{gold_table}'\")\n",
    "    df.write.format(\"delta\").mode(\"append\").saveAsTable(GOLD_META_TABLE)\n",
    "\n",
    "\n",
    "def get_gold_last_processed(gold_table):\n",
    "    df = spark.sql(f\"SELECT last_processed_timestamp FROM {GOLD_META_TABLE} WHERE gold_table='{gold_table}'\")\n",
    "    return df.first().last_processed_timestamp if df.count() else None\n",
    "\n",
    "\n",
    "def incremental_source_df(silver_table, last_proc_ts):\n",
    "    \"\"\"Return incremental slice from silver using load_timestamp.\"\"\"\n",
    "    if not spark.catalog.tableExists(silver_table):\n",
    "        return None\n",
    "    df = spark.table(silver_table)\n",
    "    if \"load_date\" not in df.columns:\n",
    "        df = df.withColumn(\"load_date\", to_date(col(\"load_timestamp\")))\n",
    "    if last_proc_ts is None:\n",
    "        return df\n",
    "    return df.filter(col(\"load_timestamp\") > lit(last_proc_ts))\n",
    "\n",
    "\n",
    "# ---------------- GOLD MODEL TRANSFORMS ----------------\n",
    "\n",
    "def transform_daily_loan_portfolio(loans_df, payments_df, recoveries_df):\n",
    "    df = loans_df.select(\"loan_id\",\"principal\",\"roi_annual\",\"emi_amount\",\"status\")\n",
    "\n",
    "    if payments_df is not None:\n",
    "        p = payments_df.groupBy(\"loan_id\").agg(F_sum(\"amount\").alias(\"sum_payments\"))\n",
    "        df = df.join(p, \"loan_id\", \"left\")\n",
    "    else:\n",
    "        df = df.withColumn(\"sum_payments\", lit(0))\n",
    "\n",
    "    if recoveries_df is not None:\n",
    "        r = recoveries_df.groupBy(\"loan_id\").agg(F_sum(\"amount_recovered\").alias(\"sum_recoveries\"))\n",
    "        df = df.join(r, \"loan_id\", \"left\")\n",
    "    else:\n",
    "        df = df.withColumn(\"sum_recoveries\", lit(0))\n",
    "\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    df = df.withColumn(\"outstanding\", col(\"principal\") - col(\"sum_payments\") - col(\"sum_recoveries\"))\n",
    "\n",
    "    return df.agg(\n",
    "        countDistinct(\"loan_id\").alias(\"total_loans\"),\n",
    "        F_sum(when(col(\"status\")==\"ACTIVE\",1).otherwise(0)).alias(\"active_loans\"),\n",
    "        F_sum(when(col(\"status\")==\"CLOSED\",1).otherwise(0)).alias(\"closed_loans\"),\n",
    "        F_sum(\"principal\").alias(\"total_principal\"),\n",
    "        F_sum(\"outstanding\").alias(\"outstanding_principal\"),\n",
    "        (F_sum(\"roi_annual\")/countDistinct(\"loan_id\")).alias(\"avg_roi\"),\n",
    "        F_sum(\"emi_amount\").alias(\"total_emi\")\n",
    "    ).withColumn(\"load_timestamp\", current_timestamp()) \\\n",
    "     .withColumn(\"load_date\", current_date())\n",
    "\n",
    "\n",
    "def transform_customer_360(customers_df, loans_df, payments_df, bureau_df):\n",
    "    loans_agg = loans_df.groupBy(\"customer_id\").agg(\n",
    "        countDistinct(\"loan_id\").alias(\"num_loans\"),\n",
    "        F_sum(\"principal\").alias(\"total_principal\")\n",
    "    )\n",
    "\n",
    "    if payments_df is not None:\n",
    "        pay_agg = payments_df.join(loans_df.select(\"loan_id\",\"customer_id\"), \"loan_id\", \"left\") \\\n",
    "            .groupBy(\"customer_id\") \\\n",
    "            .agg(\n",
    "                F_max(\"payment_date\").alias(\"last_payment_date\"),\n",
    "                F_sum(\"amount\").alias(\"total_paid\")\n",
    "            )\n",
    "    else:\n",
    "        pay_agg = None\n",
    "\n",
    "    if bureau_df is not None:\n",
    "        bureau_agg = bureau_df.groupBy(\"customer_id\").agg(F_sum(\"score\").alias(\"bureau_score\"))\n",
    "    else:\n",
    "        bureau_agg = None\n",
    "\n",
    "    df = customers_df.select(\n",
    "        \"customer_id\",\"full_name\",\"pan\",\"city\",\"state\",\"risk_segment\"\n",
    "    ).dropDuplicates([\"customer_id\"])\n",
    "\n",
    "    df = df.join(loans_agg, \"customer_id\", \"left\")\n",
    "\n",
    "    if pay_agg is not None:\n",
    "        df = df.join(pay_agg, \"customer_id\", \"left\")\n",
    "    else:\n",
    "        df = df.withColumn(\"last_payment_date\", lit(None)).withColumn(\"total_paid\", lit(0))\n",
    "\n",
    "    if bureau_agg is not None:\n",
    "        df = df.join(bureau_agg, \"customer_id\", \"left\")\n",
    "    else:\n",
    "        df = df.withColumn(\"bureau_score\", lit(None))\n",
    "\n",
    "    return df.withColumn(\"load_timestamp\", current_timestamp()) \\\n",
    "             .withColumn(\"load_date\", current_date())\n",
    "\n",
    "\n",
    "def transform_delinquency_buckets(collections_df):\n",
    "    df = collections_df.withColumn(\"dpd_bucket\",\n",
    "        when(col(\"dpd\") <= 30, \"0-30\")\n",
    "        .when((col(\"dpd\") > 30) & (col(\"dpd\") <= 60), \"31-60\")\n",
    "        .when((col(\"dpd\") > 60) & (col(\"dpd\") <= 90), \"61-90\")\n",
    "        .otherwise(\"90+\")\n",
    "    )\n",
    "    return df.groupBy(\"dpd_bucket\") \\\n",
    "             .agg(\n",
    "                 countDistinct(\"loan_id\").alias(\"loan_count\"),\n",
    "                 F_sum(\"dpd\").alias(\"dpd_sum\")\n",
    "             ).withColumn(\"load_timestamp\", current_timestamp()) \\\n",
    "              .withColumn(\"load_date\", current_date())\n",
    "\n",
    "\n",
    "def transform_payments_summary(payments_df):\n",
    "    return payments_df.groupBy(\"payment_date\",\"status\") \\\n",
    "        .agg(\n",
    "            F_sum(\"amount\").alias(\"total_amount\"),\n",
    "            countDistinct(\"payment_id\").alias(\"num_payments\")\n",
    "        ).withColumn(\"load_timestamp\", current_timestamp()) \\\n",
    "         .withColumn(\"load_date\", current_date())\n",
    "\n",
    "\n",
    "def transform_recoveries_summary(recoveries_df):\n",
    "    return recoveries_df.groupBy(\"event_date\",\"channel\") \\\n",
    "        .agg(\n",
    "            F_sum(\"amount_recovered\").alias(\"total_recovered\"),\n",
    "            countDistinct(\"recovery_id\").alias(\"num_recoveries\")\n",
    "        ).withColumn(\"load_timestamp\", current_timestamp()) \\\n",
    "         .withColumn(\"load_date\", current_date())\n",
    "\n",
    "\n",
    "# ---------------- UPSERT WITH OPTIMIZE + ZORDER ----------------\n",
    "def upsert_gold(gold_df, gold_table_full, business_keys):\n",
    "    if gold_df is None:\n",
    "        return 0\n",
    "    rows = gold_df.count()\n",
    "    if rows == 0:\n",
    "        return 0\n",
    "\n",
    "    # CREATE TABLE IF MISSING\n",
    "    if not spark.catalog.tableExists(gold_table_full):\n",
    "        print(f\"Creating Gold table {gold_table_full}\")\n",
    "        gold_df.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"mergeSchema\",\"true\") \\\n",
    "            .partitionBy(GOLD_PARTITION_COL) \\\n",
    "            .saveAsTable(gold_table_full)\n",
    "\n",
    "        try:\n",
    "            print(f\"OPTIMIZE + ZORDER {gold_table_full}\")\n",
    "            spark.sql(f\"OPTIMIZE {gold_table_full} ZORDER BY ({', '.join(business_keys)})\")\n",
    "        except Exception as e:\n",
    "            print(\"Optimize skipped:\", e)\n",
    "\n",
    "        return rows\n",
    "\n",
    "    # MERGE\n",
    "    gold_df.createOrReplaceTempView(\"gold_stg\")\n",
    "\n",
    "    on_cond = \" AND \".join([f\"t.`{k}` = s.`{k}`\" for k in business_keys])\n",
    "    cols = gold_df.columns\n",
    "\n",
    "    set_clause = \", \".join([f\"t.`{c}` = s.`{c}`\" for c in cols if c not in business_keys])\n",
    "    insert_cols = \", \".join([f\"`{c}`\" for c in cols])\n",
    "    insert_vals = \", \".join([f\"s.`{c}`\" for c in cols])\n",
    "\n",
    "    merge_sql = f\"\"\"\n",
    "        MERGE INTO {gold_table_full} t\n",
    "        USING gold_stg s\n",
    "        ON {on_cond}\n",
    "        WHEN MATCHED THEN UPDATE SET {set_clause}\n",
    "        WHEN NOT MATCHED THEN INSERT ({insert_cols}) VALUES ({insert_vals})\n",
    "    \"\"\"\n",
    "\n",
    "    spark.sql(merge_sql)\n",
    "\n",
    "    # OPTIMIZE + ZORDER\n",
    "    try:\n",
    "        print(f\"OPTIMIZE + ZORDER on {gold_table_full}\")\n",
    "        spark.sql(f\"OPTIMIZE {gold_table_full} ZORDER BY ({', '.join(business_keys)})\")\n",
    "    except Exception as e:\n",
    "        print(\"Optimize skipped:\", e)\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "# ---------------- MAIN ETL ----------------\n",
    "def gold_etl_run():\n",
    "    ensure_gold_control_tables()\n",
    "    run_id = str(uuid.uuid4())\n",
    "    start_ts = now_ts()\n",
    "\n",
    "    print(f\"\\n===== Silver → Gold ETL START (run_id={run_id}) =====\\n\")\n",
    "\n",
    "    # 1. Daily Loan Portfolio\n",
    "    try:\n",
    "        gold = f\"{CATALOG}.{SCHEMA}.daily_loan_portfolio_gold\"\n",
    "        last = get_gold_last_processed(gold)\n",
    "\n",
    "        loans      = incremental_source_df(SILVER[\"loans\"], last)\n",
    "        payments   = incremental_source_df(SILVER[\"payments\"], last)\n",
    "        recoveries = incremental_source_df(SILVER[\"recoveries\"], last)\n",
    "\n",
    "        if loans is not None and loans.count() > 0:\n",
    "            df = transform_daily_loan_portfolio(loans, payments, recoveries)\n",
    "            rows = upsert_gold(df, gold, [\"load_date\"])\n",
    "            max_ts = loans.agg(F_max(\"load_timestamp\")).first()[0]\n",
    "            update_gold_meta(gold, max_ts)\n",
    "            write_gold_runlog(run_id, gold, start_ts, now_ts(), \"SUCCESS\", rows, \"Portfolio refreshed\")\n",
    "        else:\n",
    "            write_gold_runlog(run_id, gold, start_ts, now_ts(), \"NO_DATA\", 0, \"No new loans\")\n",
    "    except Exception as e:\n",
    "        write_gold_runlog(run_id, \"daily_loan_portfolio_gold\", start_ts, now_ts(), \"FAILED\", 0, str(e))\n",
    "\n",
    "    # 2. Customer 360\n",
    "    try:\n",
    "        gold = f\"{CATALOG}.{SCHEMA}.customer_360_gold\"\n",
    "        last = get_gold_last_processed(gold)\n",
    "\n",
    "        loans      = incremental_source_df(SILVER[\"loans\"], last)\n",
    "        customers  = incremental_source_df(SILVER[\"customers\"], last)\n",
    "        payments   = incremental_source_df(SILVER[\"payments\"], last)\n",
    "        bureau     = incremental_source_df(SILVER[\"bureau\"], last)\n",
    "\n",
    "        if loans is not None and loans.count() > 0:\n",
    "            df = transform_customer_360(customers, loans, payments, bureau)\n",
    "            rows = upsert_gold(df, gold, [\"customer_id\"])\n",
    "            max_ts = loans.agg(F_max(\"load_timestamp\")).first()[0]\n",
    "            update_gold_meta(gold, max_ts)\n",
    "            write_gold_runlog(run_id, gold, start_ts, now_ts(), \"SUCCESS\", rows, \"Customer 360 refreshed\")\n",
    "        else:\n",
    "            write_gold_runlog(run_id, gold, start_ts, now_ts(), \"NO_DATA\", 0, \"No new loans\")\n",
    "    except Exception as e:\n",
    "        write_gold_runlog(run_id, \"customer_360_gold\", start_ts, now_ts(), \"FAILED\", 0, str(e))\n",
    "\n",
    "    # 3. Delinquency Buckets\n",
    "    try:\n",
    "        gold = f\"{CATALOG}.{SCHEMA}.delinquency_buckets_gold\"\n",
    "        last = get_gold_last_processed(gold)\n",
    "\n",
    "        collections = incremental_source_df(SILVER[\"collections\"], last)\n",
    "\n",
    "        if collections is not None and collections.count() > 0:\n",
    "            df = transform_delinquency_buckets(collections)\n",
    "            rows = upsert_gold(df, gold, [\"dpd_bucket\"])\n",
    "            max_ts = collections.agg(F_max(\"load_timestamp\")).first()[0]\n",
    "            update_gold_meta(gold, max_ts)\n",
    "            write_gold_runlog(run_id, gold, start_ts, now_ts(), \"SUCCESS\", rows, \"Delinquency refreshed\")\n",
    "        else:\n",
    "            write_gold_runlog(run_id, gold, start_ts, now_ts(), \"NO_DATA\", 0, \"No collections\")\n",
    "    except Exception as e:\n",
    "        write_gold_runlog(run_id, \"delinquency_buckets_gold\", start_ts, now_ts(), \"FAILED\", 0, str(e))\n",
    "\n",
    "    # 4. Payments Summary\n",
    "    try:\n",
    "        gold = f\"{CATALOG}.{SCHEMA}.payments_summary_gold\"\n",
    "        last = get_gold_last_processed(gold)\n",
    "\n",
    "        payments = incremental_source_df(SILVER[\"payments\"], last)\n",
    "\n",
    "        if payments is not None and payments.count() > 0:\n",
    "            df = transform_payments_summary(payments)\n",
    "            rows = upsert_gold(df, gold, [\"payment_date\",\"status\"])\n",
    "            max_ts = payments.agg(F_max(\"load_timestamp\")).first()[0]\n",
    "            update_gold_meta(gold, max_ts)\n",
    "            write_gold_runlog(run_id, gold, start_ts, now_ts(), \"SUCCESS\", rows, \"Payments summary refreshed\")\n",
    "        else:\n",
    "            write_gold_runlog(run_id, gold, start_ts, now_ts(), \"NO_DATA\", 0, \"No payments\")\n",
    "    except Exception as e:\n",
    "        write_gold_runlog(run_id, \"payments_summary_gold\", start_ts, now_ts(), \"FAILED\", 0, str(e))\n",
    "\n",
    "    # 5. Recoveries Summary\n",
    "    try:\n",
    "        gold = f\"{CATALOG}.{SCHEMA}.recoveries_summary_gold\"\n",
    "        last = get_gold_last_processed(gold)\n",
    "\n",
    "        recoveries = incremental_source_df(SILVER[\"recoveries\"], last)\n",
    "\n",
    "        if recoveries is not None and recoveries.count() > 0:\n",
    "            df = transform_recoveries_summary(recoveries)\n",
    "            rows = upsert_gold(df, gold, [\"event_date\",\"channel\"])\n",
    "            max_ts = recoveries.agg(F_max(\"load_timestamp\")).first()[0]\n",
    "            update_gold_meta(gold, max_ts)\n",
    "            write_gold_runlog(run_id, gold, start_ts, now_ts(), \"SUCCESS\", rows, \"Recoveries summary refreshed\")\n",
    "        else:\n",
    "            write_gold_runlog(run_id, gold, start_ts, now_ts(), \"NO_DATA\", 0, \"No recoveries\")\n",
    "    except Exception as e:\n",
    "        write_gold_runlog(run_id, \"recoveries_summary_gold\", start_ts, now_ts(), \"FAILED\", 0, str(e))\n",
    "\n",
    "    print(f\"\\n===== Silver → Gold ETL END (run_id={run_id}) =====\")\n",
    "\n",
    "\n",
    "# Execute the ETL\n",
    "gold_etl_run()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ETLSilver2Gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
